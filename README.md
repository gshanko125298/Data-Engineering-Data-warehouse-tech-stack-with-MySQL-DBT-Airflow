# Data-Engineering-Data-warehouse-tech-stack-with-MySQL-DBT-Airflow 
To modifay the  created an AI startup  on the way to fits that deploys sensors to businesses, collects data from all activities in a business - people’s interaction, traffic flows, smart appliances installed in a company. The intended A startup helps organisations obtain critical intelligence based on public and private data they collect and organise. 


![image](https://user-images.githubusercontent.com/43541659/200939695-b042d00b-a6b1-4a59-966e-1f51a49000a9.png)

# Tools used in the project
some of the tools and its documention link listed below

# Apache Airflow!
Apache Airflow Core, which includes webserver, scheduler, CLI and other components that are needed for minimal Airflow installation. Read the documentation »
Airflow is a platform to programmatically author, schedule and monitor workflows.

Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.

When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.

#DAGS
